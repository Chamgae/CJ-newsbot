import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from datetime import datetime, timedelta

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(
    page_title="í™ë³´íŒ€ ë‰´ìŠ¤ í†µí•© ìˆ˜ì§‘ê¸°",
    page_icon="ğŸ“°",
    layout="wide"
)

# --- 1. ë‰´ìŠ¤ ìƒì„¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ë³¸ë¬¸ ì¶”ì¶œ) ---
def get_news_content(url):
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"}
    try:
        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            return None
            
        soup = BeautifulSoup(response.text, "html.parser")
        
        # ì œëª© ì¶”ì¶œ
        title = soup.select_one("#title_area span") or soup.select_one(".media_end_head_headline")
        title = title.get_text(strip=True) if title else ""
        
        # ë‚ ì§œ ì¶”ì¶œ
        date = soup.select_one(".media_end_head_info_datestamp_time")
        date = date.get_text(strip=True) if date else ""
        
        # ì–¸ë¡ ì‚¬ ì¶”ì¶œ
        press = soup.select_one(".media_end_linked_more_point")
        press = press.get_text(strip=True) if press else ""
        
        # ë³¸ë¬¸ ì¶”ì¶œ
        content = soup.select_one("#dic_area") or soup.select_one("#newsct_article")
        if content:
            # ì´ë¯¸ì§€ ì„¤ëª…, ìš”ì•½ ë“± ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
            for tag in content.select(".img_desc, .media_end_summary, .guide_text"):
                tag.extract()
            body = content.get_text(separator="\n").strip()
        else:
            body = "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"
            
        return {
            "ì‘ì„±ì¼": date,
            "ì–¸ë¡ ì‚¬": press,
            "ì œëª©": title,
            "ë³¸ë¬¸": body,
            "ë§í¬": url
        }
    except Exception:
        return None

# --- 2. ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ í¬ë¡¤ë§ ---
def crawl_naver_news(keyword, start_date, end_date, max_pages):
    results = []
    
    # ë„¤ì´ë²„ ê²€ìƒ‰ ë‚ ì§œ í¬ë§· ë³€í™˜
    sd_dot = start_date.strftime("%Y.%m.%d")
    ed_dot = end_date.strftime("%Y.%m.%d")
    sd_raw = start_date.strftime("%Y%m%d")
    ed_raw = end_date.strftime("%Y%m%d")
    
    headers = {"User-Agent": "Mozilla/5.0"}
    
    # ì§„í–‰ ìƒí™© í‘œì‹œë°”
    bar = st.progress(0)
    status = st.empty()
    
    for i in range(max_pages):
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        status.text(f"ğŸ” {i+1}í˜ì´ì§€ ê²€ìƒ‰ ì¤‘... (í˜„ì¬ {len(results)}ê±´ ìˆ˜ì§‘ë¨)")
        bar.progress((i + 1) / max_pages)
        
        # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ URL ìƒì„±
        start_idx = i * 10 + 1
        url = f"https://search.naver.com/search.naver?where=news&query={keyword}&sm=tab_opt&sort=0&photo=0&field=0&pd=3&ds={sd_dot}&de={ed_dot}&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom{sd_raw}to{ed_raw}&is_sug_officeid=0&office_category=0&service_area=0&start={start_idx}"
        
        res = requests.get(url, headers=headers)
        soup = BeautifulSoup(res.text, "html.parser")
        
        # ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        articles = soup.select("div.news_wrap")
        
        if not articles:
            break # ê¸°ì‚¬ê°€ ë” ì—†ìœ¼ë©´ ì¤‘ë‹¨
        
        for article in articles:
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ì¸ë§í¬(n.news.naver.com)ë§Œ í•„í„°ë§
            links = article.select("a.info")
            for link in links:
                href = link.attrs.get("href", "")
                if "n.news.naver.com" in href:
                    # ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§
                    data = get_news_content(href)
                    if data:
                        results.append(data)
                    break # ë™ì¼ ê¸°ì‚¬ëŠ” í•œ ë²ˆë§Œ ì²˜ë¦¬
        
        time.sleep(1) # ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•´ 1ì´ˆ ëŒ€ê¸°
        
    bar.empty()
    status.empty()
    return pd.DataFrame(results)

# --- UI í™”ë©´ êµ¬ì„± ---
st.title("ğŸ—ï¸ í™ë³´íŒ€ ë‰´ìŠ¤ í†µí•© ìˆ˜ì§‘ê¸°")
st.markdown("íŠ¹ì • ê¸°ê°„ì˜ í‚¤ì›Œë“œ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ì—¬ **ì—‘ì…€(CSV)**ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.")

# ì…ë ¥ì°½ ë°°ì¹˜
col1, col2 = st.columns([3, 1])
keyword = col1.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ", placeholder="ì˜ˆ: ì‚¼ì„±ì „ì, ESG ê²½ì˜")
pages = col2.number_input("ê²€ìƒ‰í•  í˜ì´ì§€ ìˆ˜ (1í˜ì´ì§€=10ê±´)", min_value=1, max_value=50, value=3)

col3, col4 = st.columns(2)
s_date = col3.date_input("ì‹œì‘ì¼", value=datetime.now() - timedelta(days=1))
e_date = col4.date_input("ì¢…ë£Œì¼", value=datetime.now())

# ì‹¤í–‰ ë²„íŠ¼
if st.button("ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì—‘ì…€ ë³€í™˜", type="primary"):
    if not keyword:
        st.warning("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            df = crawl_naver_news(keyword, s_date, e_date, pages)
            
        if df.empty:
            st.error("ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ê°„ì´ë‚˜ í‚¤ì›Œë“œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        else:
            st.success(f"ì™„ë£Œ! ì´ {len(df)}ê±´ì˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
            
            # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
            st.dataframe(df)
            
            # ì—‘ì…€(CSV) ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
            csv = df.to_csv(index=False).encode('utf-8-sig')
            st.download_button(
                label="ğŸ“¥ ì—‘ì…€(CSV) ë‹¤ìš´ë¡œë“œ",
                data=csv,
                file_name=f"{keyword}_ë‰´ìŠ¤ëª¨ë‹ˆí„°ë§.csv",
                mime="text/csv"
            )
